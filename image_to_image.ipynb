{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "image_to_image.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qhNJ8ulfx8JG",
        "VuqnpxgJ1ogM"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELK8U2VCDMtN"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q71uadXqx8JC"
      },
      "source": [
        "import math\n",
        "from random import sample\n",
        "from random import randint\n",
        "\n",
        "!pip install comet-ml &> /dev/null\n",
        "import comet_ml\n",
        "\n",
        "!pip install pytorch-lightning &> /dev/null\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CometLogger\n",
        "import tensorboard\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "\n",
        "!pip install pytorch-msssim &> /dev/null\n",
        "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
        "\n",
        "!pip install dlib &> /dev/null\n",
        "import dlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcIpkAP2x8JD"
      },
      "source": [
        "# Preprocessing  (1.1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y3lIONBx8JE"
      },
      "source": [
        "def load_video(filepath, start_frame=0, end_frame=-1):\n",
        "    # import video\n",
        "    video = cv2.VideoCapture(filepath)\n",
        "    # get frame size (to size array) and number of frames\n",
        "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    # https://www.tutorialkart.com/opencv/python/opencv-python-resize-image/ resizing image\n",
        "    scale_percent = 50\n",
        "    width = int(frame_width * scale_percent / 100)\n",
        "    height = int(frame_height * scale_percent / 100)\n",
        "    dim = (width, height)\n",
        "    \n",
        "    if end_frame == -1 or end_frame > frame_count:\n",
        "        end_frame = frame_count\n",
        "\n",
        "    frames = np.empty((end_frame-start_frame, height, width, 3), np.dtype('uint8'))\n",
        "\n",
        "    for frame in range(start_frame,min(end_frame,frame_count)):\n",
        "        success, img = video.read()\n",
        "        if not success: break\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # reduce image size to help model memory\n",
        "        frames[frame] = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    video.release()\n",
        "    return frames\n",
        "\n",
        "def load_video_random_frames(filepath, number_of_frames):\n",
        "    # import video\n",
        "    video = cv2.VideoCapture(filepath)\n",
        "    # get frame size (to size array) and number of frames\n",
        "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    # https://www.tutorialkart.com/opencv/python/opencv-python-resize-image/ resizing image\n",
        "    scale_percent = 10 # 120x72 image size\n",
        "    width = int(frame_width * scale_percent / 100)\n",
        "    height = int(frame_height * scale_percent / 100)\n",
        "    dim = (width, height)                  \n",
        "    \n",
        "    frames = np.empty((number_of_frames, height, width, 3), np.dtype('uint8'))\n",
        "    \n",
        "    used_frames = []\n",
        "    for frame in range(number_of_frames):\n",
        "        num = randint(0,frame_count-1)\n",
        "        while True:\n",
        "            if num not in used_frames:\n",
        "                used_frames.append(num)\n",
        "                break\n",
        "            num = randint(0,frame_count-1)\n",
        "        video.set(cv2.CAP_PROP_POS_FRAMES,num);\n",
        "        success, img = video.read()\n",
        "        if not success: break\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # reduce image size to help model memory\n",
        "        frames[frame] = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
        "                     \n",
        "    video.release()\n",
        "    return frames"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li959gprx8JF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "MAFIA_FILEPATH = \"/content/drive/My Drive/MafiaVideogame.mp4\"\n",
        "GODFATHER_FILEPATH = \"/content/drive/My Drive/TheGodfather.mp4\"\n",
        "IRISHMAN_FILEPATH = \"/content/drive/My Drive/TheIrishman.mp4\"\n",
        "SOPRANOS_FILEPATH = \"/content/drive/My Drive/TheSopranos.mp4\"\n",
        "d = display.display(None, display_id=True)\n",
        "MAFIA_FRAMES = load_video_random_frames(MAFIA_FILEPATH,10)\n",
        "\n",
        "\n",
        "#for count, frame in enumerate(MAFIA_FRAMES):\n",
        "#    face, success = extract_face(frame)\n",
        "#    if success:\n",
        "#        d.update(Image.fromarray(face))\n",
        "#    else:\n",
        "#        print(f\"No face for frame {count}\")\n",
        "\n",
        "#GODFATHER_FRAMES = load_video_random_frames(GODFATHER_FILEPATH)\n",
        "#IRISHMAN_FRAMES = load_video_random_frames(IRISHMAN_FILEPATH)\n",
        "#SOPRANOS_FRAMES = load_video_random_frames(SOPRANOS_FILEPATH)\n",
        "\n",
        "\n",
        "# view frames\n",
        "\n",
        "#for count, frame in enumerate(MAFIA_FRAMES):\n",
        "#    d.update(Image.fromarray(frame))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byvKlQpSZjm7"
      },
      "source": [
        "#Preprocessing (Faces 3.1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Fr6aQHZrsQ"
      },
      "source": [
        "def within_bounds(centre, width, height, frame_size):\n",
        "    max_height, max_width = frame_size[0], frame_size[1]\n",
        "    if centre.x - width//2 < 0 or centre.x + width//2 >= max_width:\n",
        "        return False\n",
        "    if centre.y - height//2 < 0 or centre.y + height//2 >= max_height:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def extract_face(frame):\n",
        "    dim = (120, 72) \n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    dets, scores, _ = detector.run(frame, 1, -1)\n",
        "    for img, score in zip(dets, scores):\n",
        "        if score < 0: continue\n",
        "        # location and size of the face\n",
        "        centre, width, height = img.dcenter(), img.width(), img.height()\n",
        "        print()\n",
        "        if within_bounds(centre, width, height, np.shape(frame)):\n",
        "            crop = frame[centre.y-height//2:centre.y+height//2, centre.x-width//2:centre.x+width//2]\n",
        "            return cv2.resize(crop, dim, interpolation=cv2.INTER_AREA), True\n",
        "    return None, False"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhNJ8ulfx8JG"
      },
      "source": [
        "# Frame-to-Frame Model (2.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAUjvVEt1l91"
      },
      "source": [
        "##Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl-Nc4NA9rZJ"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        channels = 3\n",
        "        nf = 64\n",
        "        \n",
        "        def downsample_convolution(in_features, out_features):\n",
        "            return nn.Sequential(nn.Conv2d(in_features, out_features, 4, 2, 1),\n",
        "                        nn.InstanceNorm2d(out_features*2),\n",
        "                        nn.LeakyReLU(0.2))\n",
        "        \n",
        "        def residual_convolution(in_features):\n",
        "             return nn.Sequential(nn.Conv2d(in_features, in_features, 3, 1, 1),\n",
        "                                  nn.Conv2d(in_features, in_features, 3, 1, 1))\n",
        "        \n",
        "        def upsample_convolution(in_features, out_features):\n",
        "            return nn.Sequential(nn.ConvTranspose2d(in_features, out_features, 4, 2, 1),\n",
        "                        nn.InstanceNorm2d(out_features, 0.8),\n",
        "                        nn.ReLU())\n",
        "            \n",
        "            \n",
        "        self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(channels, nf, 4, 2, 1),\n",
        "                    nn.LeakyReLU())\n",
        "        self.downsample1 = downsample(nf,nf*2)\n",
        "        self.residual1 = residual_convolution(nf*2)\n",
        "        \n",
        "        self.downsample2 = downsample(nf*2,nf*4)\n",
        "        self.residual2 = residual_convolution(nf*4)\n",
        "        self.upsample1 = upsample_convolution(nf*4, nf*2)\n",
        "        \n",
        "        self.residual3 = residual_convolution(nf*2)\n",
        "        self.upsample2 = upsample_convolution(nf*2, nf)\n",
        "        self.conv2 = nn.Sequential(\n",
        "                    nn.ConvTranspose2d(nf, channels, 4, 2, 1),\n",
        "                    nn.Tanh())\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        pre_residual1_x = self.downsample1(x)\n",
        "        x = torch.cat([pre_residual1_x, self.residual1(pre_residual1_x)], dim=1)\n",
        "        pre_residual2_x = self.downsample2(x)\n",
        "        x = torch.cat([pre_residual2_x, self.residual2(pre_residual2_x)], dim=1)\n",
        "        pre_residual3_x = self.upsample1(x)\n",
        "        x = torch.cat([pre_residual3_x, self.residual3(pre_residual3_x)], dim=1)\n",
        "        x = self.upsample2(x)\n",
        "        x = self.conv2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuqnpxgJ1ogM"
      },
      "source": [
        "##Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T28GQk3Y9tXc"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator,self).__init__()\n",
        "        nf = 64\n",
        "        channels = 3\n",
        "        \n",
        "        def dilated_convolution(in_features, out_features, dilation):\n",
        "            return nn.Sequential(nn.Conv2d(in_features, out_features, 4, 1, dilation, dilation),\n",
        "                            nn.InstanceNorm2d(out_features),\n",
        "                            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        def convolution(in_features, out_features, kernal_size=4, stride=2, padding=1):\n",
        "            return nn.Sequential(nn.Conv2d(in_features, out_features, kernal_size, stride, padding),\n",
        "                            nn.InstanceNorm2d(out_features),\n",
        "                            nn.LeakyReLU(0.2, inplace=True))\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "                    nn.Conv2d(channels, nf, 4, 2, 1),\n",
        "                    nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.layer2 = convolution(nf, nf*2)\n",
        "        self.layer3 = convolution(nf*2, nf*4, 3, 1 , 1)\n",
        "        self.layer4 = dilated_convolution(nf*4, nf*4, 2)\n",
        "        self.layer5 = dilated_convolution(nf*4, nf*4, 4)\n",
        "        self.layer7 = convolution(nf*8, nf*4, 3, 1, 1)\n",
        "        self.layer8 = nn.Conv2d(nf*4, 1, 3, 1, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        layer1 = self.layer1(x)\n",
        "        layer2 = self.layer2(layer1)\n",
        "        layer3 = self.layer3(layer2)\n",
        "        layer4 = self.layer4(layer3)\n",
        "        layer5 = self.layer5(layer4)\n",
        "        layer6 = torch.cat([layer3,layer5], dim=1)\n",
        "        layer7 = self.layer7(layer6)\n",
        "        layer8 = self.layer8(layer7)\n",
        "        \n",
        "        return layer8, (layer2, layer3, layer4, layer5, layer7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddgNAd4xWPSL"
      },
      "source": [
        "##Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT-cqX7BWVfk"
      },
      "source": [
        "class GameMovieGAN(pl.LightningModule):\n",
        "    def __init__(self, mov_2_game_G, game_2_mov_G, mov_D, game_D, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.automatic_optimization = False\n",
        "        self.mov_2_game_G = mov_2_game_G\n",
        "        self.game_2_mov_G = game_2_mov_G\n",
        "        self.mov_D = mov_D\n",
        "        self.game_D = game_D\n",
        "        self.ms_ssim = MS_SSIM(data_range=255, size_average=True, channel=3)\n",
        "        self.diss_loss = nn.BCELoss()\n",
        "        self.lambda_adversary = 0.65\n",
        "        self.lambda_cycle = 0.35\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        mov_2_game_G_opt = torch.optim.Adam(self.mov_2_game_G.parameters(), lr=self.learning_rate)\n",
        "        game_2_mov_G_opt = torch.optim.Adam(self.game_2_mov_G.parameters(), lr=self.learning_rate)\n",
        "        game_D_opt = torch.optim.Adam(self.mov_2_game_D.parameters(), lr=self.learning_rate)\n",
        "        mov_D_opt = torch.optim.Adam(self.game_2_mov_D.parameters(), lr=self.learning_rate)\n",
        "        return mov_2_game_G_opt, game_2_mov_G_opt, game_D_opt, mov_D_opt\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        mov_2_game_G_opt, mov_2_game_D_opt, game_D_opt, mov_D_opt = self.optimizers()\n",
        "        true_game_img, true_movie_img = batch # check format\n",
        "        batch_size = batch.shape[0] # likely error\n",
        "\n",
        "        # Generator Training\n",
        "        ## mov_2_game train ##\n",
        "        ## get images\n",
        "        gen_game_img = self.mov_2_game_G.forward(true_movie_img)\n",
        "        cycle_img = self.game_2_mov_G.forward(gen_game_img)\n",
        "\n",
        "        ## adversary loss\n",
        "        adversary_loss = self.game_D(gen_game_img) * self.lambda_adversary\n",
        "        ## cycle loss\n",
        "        cycle_loss = (1 - self.ms_ssim(true_movie_img, cycle_img)) * self.lambda_cycle\n",
        "        ## total loss and backprop\n",
        "        total_loss = adversary_loss + cycle_loss\n",
        "        self.log('mov_2_game_generator_loss', total_loss)\n",
        "        mov_2_game_G_opt.zero_grad()\n",
        "        self.manual_backward(total_loss)\n",
        "        mov_2_game_G_opt.step()\n",
        "\n",
        "        ## game_2_mov train ##\n",
        "        ## get images\n",
        "        gen_mov_img = self.game_2_mov_G.forward(true_game_img)\n",
        "        cycle_img = self.mov_2_game_G.forward(gen_mov_img)\n",
        "\n",
        "        ## adversary loss\n",
        "        adversary_loss = self.game_D(gen_mov_img) * self.lambda_adversary\n",
        "        ## cycle loss\n",
        "        cycle_loss = (1 - self.ms_ssim(true_game_img, cycle_img)) * self.lambda_cycle\n",
        "        ## total loss and backprop\n",
        "        total_loss = adversary_loss + cycle_loss\n",
        "        self.log('game_2_mov_generator_loss', total_loss)\n",
        "        game_2_mov_G_opt.zero_grad()\n",
        "        self.manual_backward(total_loss)\n",
        "        game_2_mov_G_opt.step()\n",
        "\n",
        "        # Discriminator Training\n",
        "        real_y = torch.ones((batch_size, 1), device=self.device)\n",
        "        fake_y = torch.zeros((batch_size, 1), device=self.device)\n",
        "\n",
        "        ## Game discriminator\n",
        "        game_real_class = self.game_D(true_game_img)\n",
        "        real_error = self.diss_loss(game_real_class, real_y)\n",
        "        game_fake_class = self.game_D(self.mov_2_game_G(true_movie_img))\n",
        "        fake_error = self.diss_loss(game_fake_class, fake_y)\n",
        "\n",
        "        total_error = real_error + fake_error\n",
        "        self.log('game_discriminator_loss', total_error)\n",
        "        game_D_opt.zero_grad()\n",
        "        self.manual_backward(total_error)\n",
        "        game_D_opt.step()\n",
        "\n",
        "        ## Movie discriminator\n",
        "        mov_real_class = self.mov_D(true_mov_img)\n",
        "        real_error = self.diss_loss(mov_real_class, real_y)\n",
        "        mov_fake_class = self.mov_D(self.game_2_mov_G(true_game_img))\n",
        "        fake_error = self.diss_loss(mov_fake_class, fake_y)\n",
        "\n",
        "        total_error = real_error + fake_error\n",
        "        self.log('movie_discriminator_loss', total_error)\n",
        "        mov_D_opt.zero_grad()\n",
        "        self.manual_backward(total_error)\n",
        "        mov_D_opt.step()\n",
        "    \n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        true_game_img, true_movie_img = val_batch # check format\n",
        "        batch_size = batch.shape[0] # likely error\n",
        "        ## mov_2_game val ##\n",
        "        ## get images\n",
        "        gen_game_img = self.mov_2_game_G.forward(true_movie_img)\n",
        "        cycle_img = self.game_2_mov_G.forward(gen_game_img)\n",
        "\n",
        "        ## adversary loss\n",
        "        adversary_loss = self.game_D(gen_game_img) * self.lambda_adversary\n",
        "        ## cycle loss\n",
        "        cycle_loss = (1 - self.ms_ssim(true_movie_img, cycle_img)) * self.lambda_cycle\n",
        "        ## total loss and backprop\n",
        "        total_loss = adversary_loss + cycle_loss\n",
        "        self.log('mov_2_game_generator_val_loss', total_loss)\n",
        "\n",
        "        ## game_2_mov train ##\n",
        "        ## get images\n",
        "        gen_mov_img = self.game_2_mov_G.forward(true_game_img)\n",
        "        cycle_img = self.mov_2_game_G.forward(gen_mov_img)\n",
        "\n",
        "        ## adversary loss\n",
        "        adversary_loss = self.game_D(gen_mov_img) * self.lambda_adversary\n",
        "        ## cycle loss\n",
        "        cycle_loss = (1 - self.ms_ssim(true_game_img, cycle_img)) * self.lambda_cycle\n",
        "        ## total loss and backprop\n",
        "        total_loss = adversary_loss + cycle_loss\n",
        "        self.log('game_2_mov_generator_val_loss', total_loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}